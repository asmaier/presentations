{
  "hash": "53d4243b92211aa3a013eb7eb7a297ba",
  "result": {
    "markdown": "---\ntitle: Time Series Prediction Part I\nauthor: Dr. Andreas Maier\ndate: '2022-07-21'\nformat:\n  revealjs:\n    reference-location: document\n    logo: um_logo.png\n    slide-number: true\n    footer: '<https://www.unbelievable-machine.com/>'\n    section-title-footnotes: References\n  beamer:\n    reference-location: document\n    slide-number: true\n    logo: um_logo.png\n    footer: '<https://www.unbelievable-machine.com/>'\n    section-title-footnotes: References\n    theme: Madrid\nexecute:\n  cache: true\n---\n\n## Time Series {.smaller}\n\n:::: {.columns}\n:::{.column}\n- Clustering\n    - Dynamic Time Warping\n- Segmentation    \n    - Speech recognition    \n- Classification\n- Anomaly/Outlier Detection\n- Imputation\n- **Forecasting**\n    - Trajectory prediction\n    - Stock Market prediction\n:::\n:::{.column}\n- **Univariate**\n- Multivariate\n- Zerovariate\n    - Delphi Method\n:::\n::::\n\n\n## {.center}\n> \"Prediction is the essence of intelligence, and that's what we're trying to do.\" \\\n> \\\n> -- <cite>Yann LeCun (2015)</cite> \n\n## {.center}\n> \"Prediction is very difficult, especially if it's about the future!\" \\\n> \\\n> -- <cite>Niels Bohr</cite>\n\n## {.center}\n> \"What we can't predict, we call randomness.\" \\\n> \\\n> -- <cite>Albert Einstein</cite> \n\n\n## Understanding Randomness 1\n\nRandom walk\n: cumulative sum of random numbers\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-2-output-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Understanding Randomness 2\n\nWalk of random walk\n: cumulative sum of a random walk\n\n:::{.r-stack}\n:::{.fragment}\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-3-output-1.pdf){fig-align='center'}\n:::\n:::\n\n\n:::\n:::{.r-fit-text .fragment}\nWat??\n:::\n:::{.r-fit-text .fragment style=\"background: #FFFFFF;\"}\n<https://stats.stackexchange.com/questions/580718/properties-of-cumulative-sum-of-a-random-walk>\n:::\n:::\n\n## Pearson correlation coefficient $r$ {.smaller}\n\n![](Correlation_examples2.png)\n\n- Measures linear correlation between two sets of data\n- Equal to the slope of a linear regression model (for standardized data $\\sigma_x=\\sigma_y=1$)^[[Raschka: What is the difference between Pearson R and Simple Linear Regression?](https://sebastianraschka.com/faq/docs/pearson-r-vs-linear-regr.html)]\n\n$$\n\\hat{y} = \\phi_0 + \\phi_1 x = \\phi_0 + \\frac{cov(x,y)}{\\sigma_x^2} x = \\phi_0 + r \\frac{\\sigma_x}{\\sigma_y} x \n$$\n\n\n## Autocorrelation 1 {.smaller}\nPearson coefficients between time series and time shifted version of itself\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-4-output-1.pdf){fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-4-output-2.pdf){fig-align='center'}\n:::\n:::\n\n\n## Autocorrelation 2 {.smaller}\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-5-output-1.pdf){fig-align='center'}\n:::\n:::\n\n\n<!-- The footnote here seems to break the layout. One needs to set figsize=(5,3) in the python\ncode above to fix the layout -->\n- ACF (auto correlation function) of non-stationary data decreases slowly^[[Hyndman (2021): Forecasting: principles and practice, chapter 9.1](https://otexts.com/fpp3/stationarity.html)]\n- random walk is non-stationary\n\n## Stationarity\n\nStatistical properties don't change over time\n\n![](stationarity.png) ^[[Palachy (2019): Towards Data Science | Stationarity in time series analysis](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322)]\n\n- A stationary time series has no seasonality or trend.\n- Many tools, statistical tests and models (ARIMA!) rely on stationarity.\n\n## Differencing {.smaller}\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-6-output-1.pdf){fig-align='center'}\n:::\n:::\n\n\n- Differencing of degree $d$ can help to stabilise the mean eliminating/reducing trend and seasonality.\n- Transformations such as logarithms can help to stabilise the variance\n- ACF is zero for first derivative of random walk\n\n## Partial autocorrelation 1\n\n\n- Direct linear correlation $r_k$ between $y$ and $x_k$\n    $$\n    \\hat{y} = \\phi_0 + 0 \\cdot x_1 + ... +  r_k x_k\n    $$\n- Partial correlation $p_k$ between $y$ and $x_k$\n    $$\n    \\hat{y} = \\phi_0 + \\phi_1 x_1 + ... +  p_k x_k\n    $$ \n    Dependency on $x_1, x_2, ...$ is absorbed into $\\phi_1, \\phi_2, ...$ \n\n**Partial correlation is linear correlation between $y$ and $x_k$ with\nindirect correlation to $x_1, x_2, ...$ removed.**^[[Pen State: STAT 510 |\nApplied Time Series Analysis, chapter 2.2](https://online.stat.psu.edu/stat510/lesson/2/2.2)] \n\n## Partial autocorrelation 2 {.smaller}\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](timeseries_files/figure-beamer/cell-7-output-1.pdf){fig-align='center'}\n:::\n:::\n\n\n- PACF (Partial autocorellation function) is zero for random walk except for lag 1 \n\n## Autoregression model {.smaller}\n\n- forecast $y_t$ using a linear combination of past values\n    $$\n    \\hat{y}_t = \\phi_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} \n    $$\n- Autoregression means linear regression with lagged version of itself (similar to autocorrelation)\n- AR(p) means $\\phi_0, ..., \\phi_p \\neq 0$\n- Autoregressive process can be used to generate data from random white noise $\\epsilon_t$ and fixed parameters $\\phi_k$\n    $$\n    y_t = \\epsilon_t + \\phi_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} \n    $$\n- For $\\phi_0 = 0$ and $\\phi_1=1$ an AR(1) process is a random walk\n\n\n## Moving average model {.smaller}\n- The name \"moving average\" is technically incorrect ^[[Stackexchange: why-are-maq-time-series-models-called-moving-averages](https://stats.stackexchange.com/questions/58242/why-are-maq-time-series-models-called-moving-averages)] \n- Better would be lagged error regression \n- forecast $y_t$ using a linear combination of past forecast errors\n    $$\n    \\hat{y}_t = \\theta_0 + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q} \n    $$\n- MA(q) cannot be fitted like an ordinary least square (OLS), because the forecast errors are not known\n- [Example algorithm](https://stats.stackexchange.com/questions/505873/how-to-manually-fit-ma1-model-with-ols): Set initial values for $\\theta_k$ and $\\epsilon_k$, then\n    - For i=1..N do\n        1. Compute error terms for all $t$: $\\epsilon_t$ = $y_t - \\hat{y}_t$\n        2. Run a regression of $y_t$ against $\\hat{y}_t$ and update $\\theta_k$\n        3. Repeat\n- Like(?) [Iteratively_reweighted_least_squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) or similar iterative process to estimate $\\theta_k$\n\n## ARIMA {.smaller}\n- Autoregressive integrated moving average model combines AR(p), differencing/integrating I(d) and MA(q) \n$$\n\\hat{y}_t = c + \\phi_1 y^{(d)}_{t-1} + ... + \\phi_p y^{(d)}_{t-p} + \\theta_1 \\epsilon_{t-1} ... + \\theta_q \\epsilon_{t-q}\n$$\n\n|Model| equivalent to|\n|-|-|\n| ARIMA(0,1,0) | Random Walk (with drift) |\n| ARIMA(0,1,1) | simple exponential smoothing ETS(A,N,N)| \n| ARIMA(1,0,0) | discrete Ornstein-Uhlenbeck process ^[[Wikipedia: Autoregressive_model](https://en.wikipedia.org/wiki/Autoregressive_model#Explicit_mean/difference_form_of_AR(1)_process)] |\n\n- To find the best fitting ARIMA model one can use the Box-Jenkins-Method\n\n## Box-Jenkins Method {.smaller}\n\n1. Make the time series stationary (e.g. standardization, differencing $d$-times, ...)\n2. Use ACF plot and PACF plot to identify the parameter $p$ and $q$\n3. Fitting the parameters of the ARIMA(p,d,q) model. This can be done with [Hannan–Rissanen (1982)](https://doi.org/10.2307/2335856) algorithm\n    1. AR(m) model (with $m > max(p, q)$) is fitted to the data\n    2. Compute error terms for all $t$: $\\epsilon_t$ = $y_t - \\hat{y}_t$\n    3. Regress $y_t$ on $y^{(d)}_{t-1},..,y^{(d)}_{t-p},\\epsilon_{t-1},...,\\epsilon_{t-q}$\n    4. To improve accurancy optinally regress again with updated $\\phi,\\theta$ from step 3\n\n    Other algorithms (maximizing likelihood) are often used in practice ^[[Brockwell, Davis (2016) Introduction to Time Series and Forecasting, chapter 5](https://eprints.ukh.ac.id/id/eprint/232/)] ^[[Hyndman (2008). Automatic Time Series Forecasting: The forecast Package for R](https://doi.org/10.18637/jss.v027.i03)]\n4. Statistical model checking (analysis of mean, variance, (partial) autocorrelation, Ljung–Box test of residuals)\n5. Once the residuals look like white noise, do the forecast\n\n**Nowadays all these steps are automated by tools like AutoARIMA etc.** \n\n\n## Why???\n\n- Why combine AR and MA ?\n    - [Wold's theorem](https://en.wikipedia.org/wiki/Wold%27s_theorem) ?\n\n- AR is analogeous to linear regression, but what is MA analogeous to outside of time series analysis?\n\n- Why is ARIMA better than AR alone? \n\n\n<!--\n## Autocorrelation add-on\n\n:::{.r-stack}\n:::{.fragment .fade-out fragment-index=0}\n$$\nc(l) = \\sum_{t=-\\infty}^{t=\\infty} f^*(t)f(t + l) \n$$\n:::\n:::{.fragment .current-visible fragment-index=0}\n$$\nc(l) = \\frac{1}{2N}\\sum_{t=-N}^{t=N} f^*(t)f(t + l) \n$$\n:::\n:::{.fragment .current-visible}\n$$\nc(l) = \\frac{1}{N}\\sum_{t=1}^{t=N} f^*(t)f(t + l) \n$$\n:::\n:::{.fragment}\n$$\nc(l) = \\frac{1}{N}\\sum_{t=1}^{t=N} z^*(l)z(t + l) \n$$\nwith z-transform to mean=0 and std=1\n$$\nz(t)= \\frac{f(t) - \\bar{f}}{\\sigma_f} \n$$\nEqual to Pearson correlation coefficient \n\nfor $x=z(t)$ and $y=z(t+l)$\n:::\n:::\n\n\n\n## Predicting a random walk\n\nIs it possible to predict a random walk?\n\nNo, but...\n\n> Anyone who attempts to generate random numbers by deterministic means is, of course, living in a state of sin. \\\n> \\\n> -- <cite>John v. Neumann</cite>\n\nCould it be possible that a huge neural network can model the deterministic\nalgorithm and figure out the seed of the random number generator?\n-->\n\n",
    "supporting": [
      "timeseries_files/figure-beamer"
    ],
    "filters": []
  }
}